\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=1in}

\title{Modeling the Probability of a Binary Outcome using Neural Networks with Cross-Entropy Loss}
\author{}
\date{}

\begin{document}
\maketitle

\section*{1. Introduction to Binary Classification with Neural Networks}

Modeling the probability of a binary outcome, such as "yes/no" or "event/no event," is a fundamental problem in statistical modeling and machine learning. When we approach this problem using neural networks, we typically aim to predict the probability of an event occurring based on one or more input features. Neural networks are powerful tools for capturing complex, non-linear relationships between input features and the probability of an outcome.

For background, see Chapter 4 of Hastie, Tibshirani, and Friedman's *The Elements of Statistical Learning* (2009) for an introduction to neural networks and the mathematical principles behind them \cite{hastie2009elements}.

\section*{2. Neural Network Structure for Binary Classification}

A neural network for binary classification consists of layers of interconnected neurons:
\begin{itemize}
    \item **Input Layer**: Takes in the predictor variables (features) of each observation.
    \item **Hidden Layers**: These intermediate layers allow the network to learn complex relationships by applying non-linear transformations.
    \item **Output Layer**: For binary classification, the output layer typically has one neuron with a sigmoid activation function, which outputs a probability between 0 and 1, representing the likelihood of the outcome.
\end{itemize}

Each neuron in a layer computes a weighted sum of inputs, adds a bias term, and applies an activation function. This transformation allows the network to learn non-linear mappings between input features and the probability of an event.

For further details, see Section 11.3 of *An Introduction to Statistical Learning with Applications in R* by James et al. (2013), which offers a concise and accessible introduction to neural networks \cite{james2013introduction}.

\section*{3. Mathematical Formulation}

Let \( X = (X_1, X_2, \dots, X_k) \) be the vector of predictor variables for a single observation, and let \( y \in \{0, 1\} \) be the binary outcome we want to predict. The neural network computes the predicted probability \( \hat{P}(y=1 | X) \) as follows:

1. **Linear Transformation**: For each layer \( l \), the input to the layer is linearly transformed. For a neuron \( j \) in layer \( l \), we have:
   \[
   z_j^{(l)} = w_j^{(l)} \cdot h^{(l-1)} + b_j^{(l)}
   \]
   where \( w_j^{(l)} \) is the weight vector for neuron \( j \) in layer \( l \), \( h^{(l-1)} \) is the output from the previous layer, and \( b_j^{(l)} \) is the bias term.

2. **Activation Function**: The linear transformation is followed by a non-linear activation function. For hidden layers, common choices include the ReLU (Rectified Linear Unit) function:
   \[
   h_j^{(l)} = \max(0, z_j^{(l)})
   \]
   For the output layer in binary classification, we use the sigmoid activation function, which ensures the output is a probability:
   \[
   \hat{P}(y=1 | X) = \frac{1}{1 + e^{-z}}
   \]
   where \( z \) is the output of the last layer before the sigmoid is applied.

\section*{4. Cross-Entropy Loss Function}

To train the neural network, we need a loss function that measures the discrepancy between the predicted probabilities \( \hat{P}(y=1 | X) \) and the actual binary outcomes \( y \). For binary classification, the **cross-entropy loss** function is commonly used. This loss function is defined as follows:

\[
\text{Cross-Entropy Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{P}(y=1 | X_i)) + (1 - y_i) \log(1 - \hat{P}(y=1 | X_i)) \right]
\]

where:
\begin{itemize}
    \item \( n \) is the number of observations,
    \item \( y_i \) is the actual binary outcome for the \( i \)-th observation,
    \item \( \hat{P}(y=1 | X_i) \) is the predicted probability of \( y = 1 \) for the \( i \)-th observation.
\end{itemize}


The cross-entropy loss function is a widely-used objective function for training neural networks in binary classification tasks. It quantifies the difference between the true labels and the predicted probabilities, penalizing the model more heavily when it assigns low probabilities to the correct class.

This function measures the average "distance" between the true distribution of labels and the predicted distribution. Specifically, it penalizes predictions that are confident but incorrect, encouraging the network to assign high probabilities to the correct class.


Cross-entropy loss has several important properties that make it ideal for training neural networks:
\begin{itemize}
    \item {\em **Sensitivity to Probability Estimates**: The loss heavily penalizes confident predictions that are wrong. For instance, if the model predicts a very low probability for the true class (close to 0), the loss will be high.}. 
To see this, for example, if $\hat{P}(y=1)\to 0 $ but $y_i=1$,
the contribution to the loss is $ -\frac{1}{n} y_i \log(\hat{P}(y=1 | X_i) \to \infty$. 
    \item **Encouragement of High Probabilities for True Class**: Minimizing the cross-entropy loss naturally drives the model to assign higher probabilities to the correct class.
    \item **Connection to Likelihood Maximization**: Cross-entropy loss is equivalent to the negative log-likelihood in a probabilistic framework. Thus, minimizing cross-entropy is akin to maximizing the likelihood of the observed data under the model.
\end{itemize}This loss function penalizes the model more heavily when it assigns low probabilities to the true class. Minimizing cross-entropy encourages the network to assign high probabilities to the correct classes.

For further explanation of cross-entropy loss, see *Deep Learning* by Goodfellow, Bengio, and Courville (2016), Section 5.5 \cite{goodfellow2016deep}.

\section*{5. Training the Neural Network}

Training a neural network involves finding the parameters (weights and biases) that minimize the cross-entropy loss. This is typically done using **gradient descent** or one of its variants (e.g., stochastic gradient descent). The algorithm proceeds as follows:

1. **Forward Pass**: Compute the predicted probabilities \( \hat{P}(y=1 | X) \) for each observation.
2. **Loss Calculation**: Calculate the cross-entropy loss based on the predicted probabilities and the true labels.
3. **Backward Pass**: Use backpropagation to compute the gradients of the loss with respect to each parameter.
4. **Parameter Update**: Update each parameter using the gradients. For gradient descent, a simple update rule is:
   \[
   \theta = \theta - \alpha \frac{\partial \text{Loss}}{\partial \theta}
   \]
   where \( \alpha \) is the learning rate, and \( \theta \) represents the network parameters.

This process iterates until convergence, meaning that the loss function stabilizes at a (local) minimum. Chapter 6 of *Neural Networks and Deep Learning* by Nielsen (2015) provides an accessible overview of gradient descent and backpropagation for training neural networks \cite{nielsen2015neural}.

\section*{6. Summary}

In summary, neural networks with a cross-entropy loss function offer a flexible approach to modeling probabilities for binary outcomes. The network architecture allows for complex, non-linear relationships between the predictors and the outcome, while the cross-entropy loss provides a mathematically principled way to train the network on binary classification tasks. This approach is powerful for applications ranging from image classification to predicting binary events in time series and beyond.

\begin{thebibliography}{9}

\bibitem{hastie2009elements}
T. Hastie, R. Tibshirani, and J. Friedman,
\textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.
Springer, 2009, Ch. 4.

\bibitem{james2013introduction}
G. James, D. Witten, T. Hastie, and R. Tibshirani,
\textit{An Introduction to Statistical Learning with Applications in R}.
Springer, 2013, Sec. 11.3.

\bibitem{goodfellow2016deep}
I. Goodfellow, Y. Bengio, and A. Courville,
\textit{Deep Learning}.
MIT Press, 2016, Sec. 5.5.

\bibitem{nielsen2015neural}
M. Nielsen,
\textit{Neural Networks and Deep Learning}.
Determination Press, 2015, Ch. 6.

\end{thebibliography}

\appendix

\section{Proof: Cross-Entropy Loss is Equivalent to Negative Log-Likelihood}

\subsection*{1. Binary Classification Setup}

Let:
\begin{itemize}
    \item \( y \in \{0, 1\} \): the true binary label for a data point,
    \item \( \hat{P}(y=1|X) \): the predicted probability that \( y=1 \), given the input features \( X \),
    \item \( \hat{P}(y=0|X) = 1 - \hat{P}(y=1|X) \): the predicted probability that \( y=0 \).
\end{itemize}

The likelihood of observing the true label \( y \), given the prediction \( \hat{P} \), is:
\[
P(y | X) = 
\begin{cases} 
\hat{P}(y=1|X), & \text{if } y = 1, \\
1 - \hat{P}(y=1|X), & \text{if } y = 0.
\end{cases}
\]

This can be expressed compactly using the binary indicator \( y \) as:
\[
P(y | X) = [\hat{P}(y=1|X)]^y \cdot [1 - \hat{P}(y=1|X)]^{1-y}.
\]

\subsection*{2. Negative Log-Likelihood}

The log-likelihood for a single data point is:
\[
\log P(y | X) = y \log(\hat{P}(y=1|X)) + (1-y) \log(1 - \hat{P}(y=1|X)).
\]

The negative log-likelihood (NLL), which is minimized during training, is:
\[
\text{NLL} = -\log P(y | X).
\]

Substituting the expression for \( \log P(y | X) \), we have:
\[
\text{NLL} = -\left[ y \log(\hat{P}(y=1|X)) + (1-y) \log(1 - \hat{P}(y=1|X)) \right].
\]

\subsection*{3. Cross-Entropy Loss}

The cross-entropy loss for a single data point is defined as:
\[
\text{Cross-Entropy Loss} = - \left[ y \log(\hat{P}(y=1|X)) + (1-y) \log(1 - \hat{P}(y=1|X)) \right].
\]

\section*{4. Equivalence of Cross-Entropy Loss and NLL}

Comparing the expressions for negative log-likelihood and cross-entropy loss:
\[
\text{NLL} = \text{Cross-Entropy Loss}.
\]

Thus, minimizing the cross-entropy loss is equivalent to minimizing the negative log-likelihood of the data under the model.

\subsection*{5. Intuition}

The equivalence arises because both cross-entropy loss and negative log-likelihood are derived from the same probabilistic framework:
\begin{itemize}
    \item The cross-entropy measures the "distance" between the true label distribution and the predicted probability distribution.
    \item The negative log-likelihood penalizes the model for assigning low probabilities to the true labels, encouraging it to maximize the likelihood of the observed data.
\end{itemize}

Both objectives lead to the same optimization process and training dynamics in binary classification.


\end{document}

