{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de770c78-c336-453a-875c-03ff17da591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neccessary packages\n",
    "## In-house packages\n",
    "from saveload import save,load\n",
    "## Pacakages from anywhere else\n",
    "import os\n",
    "from os import path as osp \n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol, xy\n",
    "from pyproj import CRS, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8afed91-74ec-4f40-8322-f7d9fafac123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file paths\n",
    "## Define the base directory (main)\n",
    "main = osp.join('C:/', 'Users', 'T-Spe', 'OneDrive', 'School', \"Fall '25\", \"Master's Project\", 'test')\n",
    "tifdata = osp.join(main, 'tifdata')\n",
    "\n",
    "## topography paths\n",
    "elevation_path = osp.join(main, 'LH20_Elev_220.tif')\n",
    "\n",
    "## fire detection (red pixel detection....)\n",
    "fire_path = osp.join(tifdata, 'ml_data')\n",
    "\n",
    "# row, col indices and mask file path\n",
    "rowcol_test_path = osp.join(main, 'row_col_and_mask_sample.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5874dd-30cb-4ba3-937c-8ddf3e0bb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcdf_with_row_col_and_mask(\n",
    "    lon_array,\n",
    "    lat_array,\n",
    "    raster_crs,\n",
    "    transform,\n",
    "    raster_shape,\n",
    "    output_file,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a NetCDF file with row, column indices, and a spatial mask for provided lon/lat data.\n",
    "    \n",
    "    Args:\n",
    "        lon_array (np.ndarray): Array of longitudes in WGS84.\n",
    "        lat_array (np.ndarray): Array of latitudes in WGS84.\n",
    "        raster_crs (str): CRS of the raster (e.g., \"EPSG:5070\").\n",
    "        transform (Affine): Rasterio affine transform of the raster.\n",
    "        raster_shape (tuple): Shape of the raster (rows, cols).\n",
    "        output_file (str): Path to the output NetCDF file.\n",
    "        debug (bool): Whether to enable debug messages.\n",
    "    \"\"\"\n",
    "    print(f\"Creating NetCDF file at {output_file}...\")\n",
    "\n",
    "    # Initialize transformer\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", raster_crs, always_xy=True)\n",
    "\n",
    "    # Reproject lon/lat to raster CRS\n",
    "    print(\"Transforming coordinates to raster CRS...\")\n",
    "    raster_lon, raster_lat = transformer.transform(lon_array, lat_array)\n",
    "\n",
    "    # Calculate row and column indices\n",
    "    inv_transform = ~transform\n",
    "    cols, rows = inv_transform * (raster_lon, raster_lat)\n",
    "\n",
    "    # Round indices and convert to integers\n",
    "    rows = np.round(rows).astype(int)\n",
    "    cols = np.round(cols).astype(int)\n",
    "\n",
    "    # Create a valid mask based on raster shape\n",
    "    valid_mask = (\n",
    "        (rows >= 0) & (rows < raster_shape[0]) &\n",
    "        (cols >= 0) & (cols < raster_shape[1])\n",
    "    )\n",
    "    rows_valid = rows[valid_mask]\n",
    "    cols_valid = cols[valid_mask]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Valid row indices: {rows_valid}\")\n",
    "        print(f\"Valid col indices: {cols_valid}\")\n",
    "        print(f\"Valid mask shape: {valid_mask.shape}\")\n",
    "        print(f\"Number of valid points: {np.sum(valid_mask)}\")\n",
    "\n",
    "    # Write to NetCDF\n",
    "    with Dataset(output_file, 'w', format='NETCDF4') as nc_file:\n",
    "        # Define dimensions\n",
    "        nc_file.createDimension('points', len(rows_valid))\n",
    "        nc_file.createDimension('mask', len(valid_mask))\n",
    "\n",
    "        # Create variables\n",
    "        rows_var = nc_file.createVariable('rows', 'i4', ('points',), zlib=True)\n",
    "        cols_var = nc_file.createVariable('cols', 'i4', ('points',), zlib=True)\n",
    "        mask_var = nc_file.createVariable('valid_mask', 'i1', ('mask',), zlib=True)\n",
    "\n",
    "        # Write data\n",
    "        rows_var[:] = rows_valid\n",
    "        cols_var[:] = cols_valid\n",
    "        mask_var[:] = valid_mask.astype(int)\n",
    "\n",
    "    print(f\"NetCDF file saved: {output_file}\")\n",
    "\n",
    "def validate_row_col(rows, cols, raster_crs, transform):\n",
    "    \"\"\"\n",
    "    Validate the row and column indices by reprojecting them back to longitude and latitude.\n",
    "\n",
    "    Args:\n",
    "        rows (np.ndarray): Array of row indices.\n",
    "        cols (np.ndarray): Array of column indices.\n",
    "        raster_crs (str): CRS of the raster (e.g., \"EPSG:5070\").\n",
    "        transform (Affine): Rasterio affine transform of the raster.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Reprojected longitude and latitude arrays.\n",
    "    \"\"\"\n",
    "    print(\"Validating row and column indices by reprojecting back to longitude and latitude...\")\n",
    "\n",
    "    # Use rasterio's transform to compute the coordinates\n",
    "    raster_coords = xy(transform, rows, cols, offset='center')\n",
    "\n",
    "    # Extract the projected coordinates\n",
    "    raster_x, raster_y = raster_coords\n",
    "\n",
    "     # Step 2: Reproject from the raster CRS to WGS84\n",
    "    transformer = Transformer.from_crs(raster_crs, \"EPSG:4326\", always_xy=True)\n",
    "    lon_wgs84, lat_wgs84 = transformer.transform(raster_x, raster_y)\n",
    "\n",
    "    #print(f\"First 10 Reprojected WGS84 Longitudes: {lon_wgs84[:10]}\")\n",
    "    #print(f\"First 10 Reprojected WGS84 Latitudes: {lat_wgs84[:10]}\")\n",
    "\n",
    "    # Verify if any NaN or invalid values exist\n",
    "    if np.isnan(lon_wgs84).any() or np.isnan(lat_wgs84).any():\n",
    "        raise ValueError(\"Reprojected coordinates contain NaN values.\")\n",
    "\n",
    "    return lon_wgs84, lat_wgs84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9c1e10-8466-4fb6-bc8c-62c6718129c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fire_detection(file_path, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Load and process fire detection data.\n",
    "    Retains all points but filters out those with a label of 1 and confidence < confidence_threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Loading fire detection data...\")\n",
    "    X, y, c, basetime = load(file_path)\n",
    "\n",
    "    # Debug: Print initial statistics\n",
    "    print(f\"Total data points: {len(X)}\")\n",
    "    print(f\"Number of 'Fire' labels: {np.sum(y == 1)}\")\n",
    "    print(f\"Number of 'Fire' labels with confidence < {confidence_threshold}: {np.sum((y == 1) & (c < confidence_threshold))}\")\n",
    "    \n",
    "    # Filter out points with label 1 and confidence < confidence_threshold\n",
    "    valid_indices = ~((y == 1) & (c < confidence_threshold))  # Keep points not failing this condition\n",
    "    X_filtered = X[valid_indices]\n",
    "    y_filtered = y[valid_indices]\n",
    "\n",
    "    # Debug: Print post-filtering statistics\n",
    "    print(f\"Number of remaining data points: {len(X_filtered)}\")\n",
    "    print(f\"Number of remaining 'Fire' labels: {np.sum(y_filtered == 1)}\")\n",
    "\n",
    "    # Extract filtered components\n",
    "    lon_array = X_filtered[:, 0]\n",
    "    lat_array = X_filtered[:, 1]\n",
    "    time_in_days = X_filtered[:, 2]\n",
    "    dates_fire_actual = basetime + pd.to_timedelta(time_in_days, unit='D')\n",
    "    dates_fire = dates_fire_actual.floor(\"h\")  # Round to nearest hour\n",
    "\n",
    "    print(f\"Loaded {len(X)} data points, filtered down to {len(X_filtered)} based on confidence and labels.\")\n",
    "    \n",
    "    return {\n",
    "        \"lon\": lon_array,\n",
    "        \"lat\": lat_array,\n",
    "        \"time_days\": time_in_days,\n",
    "        \"dates_fire\": dates_fire,\n",
    "        \"labels\": y_filtered\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388605de-9a7f-45a1-8538-836eb625c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fire detection data...\n",
      "Total data points: 1343281\n",
      "Number of 'Fire' labels: 14\n",
      "Number of 'Fire' labels with confidence < 70: 10\n",
      "Number of remaining data points: 1343271\n",
      "Number of remaining 'Fire' labels: 4\n",
      "Loaded 1343281 data points, filtered down to 1343271 based on confidence and labels.\n"
     ]
    }
   ],
   "source": [
    "#### CHECK THE CRS OF THE FILES ##### (Complete, but maybe need to make a script to make it more formal)\n",
    "## topography\n",
    "elevation_dataset = rasterio.open(elevation_path)\n",
    "elevation = elevation_dataset.read(1)\n",
    "transform = elevation_dataset.transform\n",
    "raster_crs = elevation_dataset.crs.to_string()\n",
    "raster_shape = elevation.shape\n",
    "\n",
    "# Load fire detection data\n",
    "fire_detection_data = load_fire_detection(fire_path, confidence_threshold=70)\n",
    "lon_array = fire_detection_data['lon']\n",
    "lat_array = fire_detection_data['lat']\n",
    "dates_fire = fire_detection_data['dates_fire']\n",
    "labels = fire_detection_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399ae1af-864b-49f4-bc80-b60f396bcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NetCDF file at C:/Users\\T-Spe\\OneDrive\\School\\Fall '25\\Master's Project\\test\\row_col_and_mask_sample.nc...\n",
      "Transforming coordinates to raster CRS...\n",
      "Valid row indices: [5219 5231 5243 ... 3422 3396 3411]\n",
      "Valid col indices: [4557 4497 4436 ... 3345 3334 3342]\n",
      "Valid mask shape: (1343271,)\n",
      "Number of valid points: 1162036\n",
      "NetCDF file saved: C:/Users\\T-Spe\\OneDrive\\School\\Fall '25\\Master's Project\\test\\row_col_and_mask_sample.nc\n"
     ]
    }
   ],
   "source": [
    "# Sample data and parameters\n",
    "# lon_array_test = np.array([-155.3, -155.5, -155.8])\n",
    "# lat_array_test = np.array([19.6, 19.7, 19.8])\n",
    "output_netcdf = osp.join(main, \"row_col_and_mask_sample.nc\")\n",
    "debug_mode = True\n",
    "\n",
    "# Call the function to create the saved file with row, col indices, and mask for spatial points outside of raster\n",
    "# NOTE: The saved file is there so this block does not need to be ran again.\n",
    "create_netcdf_with_row_col_and_mask(\n",
    "    lon_array,\n",
    "    lat_array,\n",
    "    raster_crs,\n",
    "    transform,\n",
    "    raster_shape,\n",
    "    output_netcdf,\n",
    "    debug=debug_mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48bf059-ba35-4cc1-bc32-1c659a902ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the mask array is: (1343271,)\n",
      "The shape of the row array is: (1162036,)\n",
      "The shape of the column array is: (1162036,)\n",
      "Mask min: 0, Mask max: 1\n",
      "Mask contains NaNs: False\n",
      "Mask contains None: False\n",
      "Rows min: 0, Rows max: 5257\n",
      "Rows contains NaNs: False\n",
      "Rows contains None: False\n",
      "Columns min: 0, Columns max: 4608\n",
      "Columns contain NaNs: False\n",
      "Columns contain None: False\n"
     ]
    }
   ],
   "source": [
    "# Read the saved test file, check the shapes and minor data exploration\n",
    "row_col_data = nc.Dataset(rowcol_test_path)\n",
    "\n",
    "# Check shapes\n",
    "mask_shape = row_col_data.variables['valid_mask'][:].shape\n",
    "rows_shape = row_col_data.variables['rows'][:].shape\n",
    "cols_shape = row_col_data.variables['cols'][:].shape\n",
    "\n",
    "print(f\"The shape of the mask array is: {mask_shape}\")\n",
    "print(f\"The shape of the row array is: {rows_shape}\")\n",
    "print(f\"The shape of the column array is: {cols_shape}\")\n",
    "\n",
    "# Check min, max, and NaN values for 'valid_mask'\n",
    "valid_mask = row_col_data.variables['valid_mask'][:]\n",
    "print(f\"Mask min: {valid_mask.min()}, Mask max: {valid_mask.max()}\")\n",
    "print(f\"Mask contains NaNs: {np.isnan(valid_mask).any()}\")\n",
    "print(f\"Mask contains None: {np.any(valid_mask == None)}\")  # NaNs should suffice, but adding for clarity\n",
    "\n",
    "# Check min, max, and NaN values for 'rows'\n",
    "rows = row_col_data.variables['rows'][:]\n",
    "print(f\"Rows min: {rows.min()}, Rows max: {rows.max()}\")\n",
    "print(f\"Rows contains NaNs: {np.isnan(rows).any()}\")\n",
    "print(f\"Rows contains None: {np.any(rows == None)}\")\n",
    "\n",
    "# Check min, max, and NaN values for 'cols'\n",
    "cols = row_col_data.variables['cols'][:]\n",
    "print(f\"Columns min: {cols.min()}, Columns max: {cols.max()}\")\n",
    "print(f\"Columns contain NaNs: {np.isnan(cols).any()}\")\n",
    "print(f\"Columns contain None: {np.any(cols == None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336f270b-150f-4e73-a10e-620d57fcae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating row and column indices by reprojecting back to longitude and latitude...\n",
      "Max Longitude Difference: 0.000287893844415521\n",
      "Max Latitude Difference: 0.00027189029520258146\n"
     ]
    }
   ],
   "source": [
    "# Reproject the (row,col) back to (lon, lat) and compare to (lon, lat) used to obtain (row, col)\n",
    "valid_mask = row_col_data.variables['valid_mask'][:].astype(bool)\n",
    "lon_array_valid = lon_array[valid_mask]\n",
    "lat_array_valid = lat_array[valid_mask]\n",
    "\n",
    "lon_reproj_row , lat_reproj_col = validate_row_col(rows, cols, raster_crs, transform)\n",
    "\n",
    "diff_lon = np.abs(lon_reproj_row - lon_array_valid)\n",
    "diff_lat = np.abs(lat_reproj_col - lat_array_valid)\n",
    "print(f\"Max Longitude Difference: {np.max(diff_lon)}\")\n",
    "print(f\"Max Latitude Difference: {np.max(diff_lat)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1359a4-7b63-46f7-bb9c-6a2649de3b60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Previous implentation of obtaining row and column indices, testing the values obtain on sampled Non-barren points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f073a-c227-4fe6-a6b6-eee38211eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import random\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol,xy\n",
    "\n",
    "def get_row_col(lon_array, lat_array, raster_crs, transform, raster_shape, debug):\n",
    "    \"\"\"\n",
    "    Optimized function to compute row and column indices for arrays of longitudes and latitudes.\n",
    "\n",
    "    Args:\n",
    "        lon_array (np.ndarray): Array of longitudes in WGS84.\n",
    "        lat_array (np.ndarray): Array of latitudes in WGS84.\n",
    "        raster_crs (str): CRS of the raster (e.g., \"EPSG:5070\").\n",
    "        transform (Affine): Rasterio affine transform of the raster.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Arrays of row and column indices in the raster.\n",
    "    \"\"\"\n",
    "    print('Computing row and column indices for topography and vegetation files...')\n",
    "\n",
    "    # Reproject lon/lat arrays to the raster's CRS using pyproj\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", raster_crs, always_xy=True)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Debug: The raster CRS is: {raster_crs}\")\n",
    "        print(f\"Debug: The raster shape is: {raster_shape}\")\n",
    "        print(f\"Debug: The transform for the raster is: {transform}\")\n",
    "        print(f\"Debug: lon_array shape: {lon_array.shape}, lat_array shape: {lat_array.shape}\")\n",
    "        print(f\"Debug: lon_array: {lon_array}\")\n",
    "        print(f\"Debug: lat_array: {lat_array}\")\n",
    "        print(f\"Debug: NaNs in lon_array: {np.isnan(lon_array).any()}, NaNs in lat_array: {np.isnan(lat_array).any()}\")\n",
    "        print(f\"Debug: lon_array min/max: {lon_array.min()} / {lon_array.max()}\")\n",
    "        print(f\"Debug: lat_array min/max: {lat_array.min()} / {lat_array.max()}\")\n",
    "        print(\"Starting coordinate transformation...\")  # Before transformer is built\n",
    "        try:\n",
    "            raster_lon, raster_lat = transformer.transform(lon_array[0], lat_array[0])\n",
    "            print(f\"Debug: Single-point transformation successful: {raster_lon}, {raster_lat}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Debug: Error during single-point transformation: {e}\")\n",
    "\n",
    "    print(\"Transforming coordinates to raster CRS...\")  # Before transformation\n",
    "    raster_lon, raster_lat = transformer.transform(lon_array, lat_array)\n",
    "    if debug:\n",
    "        print(\"Coordinate transformation completed.\")  # After transformation\n",
    "        print(\"Starting affine transformation for row/col computation...\")  # Before affine transformation\n",
    "\n",
    "    # Calculate row and column indices using vectorized transformation\n",
    "    inv_transform = ~transform\n",
    "    cols, rows = inv_transform * (raster_lon, raster_lat)\n",
    "    print(f\"The affine transformation is complete and the row,col values have been extracted...\")\n",
    "\n",
    "    # Round to nearest integer and convert to int\n",
    "    rows = np.round(rows).astype(int)\n",
    "    cols = np.round(cols).astype(int)\n",
    "\n",
    "    if debug:\n",
    "        # Debugging: Check bounds, reprojected coordinates and other metrics\n",
    "        print(f\"Debug: WGS84 lon min/max (pre-mask): {lon_array.min()} / {lon_array.max()}\")\n",
    "        print(f\"Debug: WGS84 lat min/max (pre-mask): {lat_array.min()} / {lat_array.max()}\")\n",
    "        print(f\"Debug: Reprojected lon min/max (pre-mask): {raster_lon.min()} / {raster_lon.max()}\")\n",
    "        print(f\"Debug: Reprojected lat min/max (pre-mask): {raster_lat.min()} / {raster_lat.max()}\")\n",
    "        print(f\"Debug: Rows min/max(pre-mask): {rows.min()}, {rows.max()}\")\n",
    "        print(f\"Debug: Cols min/max(pre-mask): {cols.min()}, {cols.max()}\")\n",
    "        print(f\"Debug: The shape of rows, cols: {rows.shape, cols.shape}\")\n",
    "\n",
    "    print(\"Truncating rows, cols, lon_array, and lat_array based on valid raster array inputs...\")\n",
    "\n",
    "    # Create a mask to filter valid row/col indices\n",
    "    rowcol_mask = (\n",
    "            (rows >= 0) & (rows < raster_shape[0]) &\n",
    "            (cols >= 0) & (cols < raster_shape[1])\n",
    "    )\n",
    "\n",
    "    # Calculate bounds in raster CRS\n",
    "    raster_x_min, raster_y_min = transform * (0, raster_shape[0])\n",
    "    raster_x_max, raster_y_max = transform * (raster_shape[1], 0)\n",
    "\n",
    "    # Reproject bounds to WGS84\n",
    "    transformer = Transformer.from_crs(raster_crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "    lon_min, lat_min = transformer.transform(raster_x_min, raster_y_min)\n",
    "    lon_max, lat_max = transformer.transform(raster_x_max, raster_y_max)\n",
    "\n",
    "    # Create a valid mask based on coordinate bounds via the raster boundaries\n",
    "    coord_mask = (\n",
    "            (lon_array >= lon_min) & (lon_array <= lon_max) &\n",
    "            (lat_array >= lat_min) & (lat_array <= lat_max)\n",
    "    )\n",
    "\n",
    "    # Combine the masks\n",
    "    valid_mask = rowcol_mask & coord_mask\n",
    "\n",
    "    # Apply the mask and filter spatial data accordingly\n",
    "    rows_valid = rows[valid_mask]\n",
    "    cols_valid = cols[valid_mask]\n",
    "    lon_array_valid = lon_array[valid_mask]\n",
    "    lat_array_valid = lat_array[valid_mask]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Raster bounds in WGS84: lon_min={lon_min}, lon_max={lon_max}, lat_min={lat_min}, lat_max={lat_max}\")\n",
    "        print(f\"Debug: WGS84 lon min/max (post-mask): {lon_array_valid.min()} / {lon_array_valid.max()}\")\n",
    "        print(f\"Debug: WGS84 lat min/max (post-mask): {lat_array_valid.min()} / {lat_array_valid.max()}\")\n",
    "        print(f\"Debug: Reprojected lon min/max (post-mask): {raster_x_min} / {raster_x_max}\")\n",
    "        print(f\"Debug: Reprojected lat min/max (post-mask): {raster_y_min} / {raster_y_max}\")\n",
    "        print(f\"Debug: Rows min/max(post-mask): {rows_valid.min()}, {rows_valid.max()}\")\n",
    "        print(f\"Debug: Cols min/max(post-mask): {cols_valid.min()}, {cols_valid.max()}\")\n",
    "\n",
    "    return rows_valid, cols_valid, lon_array_valid, lat_array_valid, valid_mask\n",
    "\n",
    "def sample_non_barren_points(lon_array, lat_array, fuelmod, num_samples, elevation_transform, raster_crs, raster_shape):\n",
    "    \"\"\"\n",
    "    Sample points from non-barren areas in the fuelmod raster and test both workflows.\n",
    "\n",
    "    Args:\n",
    "        lon_array (np.ndarray): Array of longitudes.\n",
    "        lat_array (np.ndarray): Array of latitudes.\n",
    "        fuelmod (np.ndarray): Categorical raster data (e.g., vegetation types).\n",
    "        num_samples (int): Number of points to sample.\n",
    "        elevation_transform: Rasterio affine transform for the elevation raster.\n",
    "        raster_crs (str): CRS of the raster (e.g., EPSG code).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Filtering non-barren areas...\")\n",
    "\n",
    "    # Identify non-barren areas\n",
    "    non_barren_mask = fuelmod != \"Barren\"\n",
    "    non_barren_indices = np.argwhere(non_barren_mask)  # Returns a 2D array of (row, col) pairs\n",
    "\n",
    "    if len(non_barren_indices) == 0:\n",
    "        print(\"No non-barren areas found in the raster.\")\n",
    "        return\n",
    "\n",
    "    if len(non_barren_indices) < num_samples:\n",
    "        print(f\"Warning: Only {len(non_barren_indices)} non-barren areas found, adjusting sample size.\")\n",
    "        num_samples = len(non_barren_indices)\n",
    "\n",
    "    # Randomly sample from non-barren indices\n",
    "    sampled_indices = random.sample(list(non_barren_indices), num_samples)\n",
    "\n",
    "    for idx, (row, col) in enumerate(sampled_indices):\n",
    "        try:\n",
    "            # Convert row, col to lon, lat for testing reverse workflow\n",
    "            lon, lat = validate_row_col(row, col, raster_crs, elevation_transform)\n",
    "            print(f\"\\nTesting sample {idx + 1}/{num_samples}\")\n",
    "            print(f\"Row: {row}, Col: {col}, Converted lon: {lon}, lat: {lat}\")\n",
    "\n",
    "            # Convert lon, lat back to row, col to validate consistency\n",
    "            test_row, test_col ,test_lon, test_lat, spatial_mask= get_row_col(np.array([lon]), np.array([lat]), raster_crs, elevation_transform, elevation.shape, False)\n",
    "            print(f\"Original row: {row}, Original col: {col}\")\n",
    "            print(f\"Recomputed row: {test_row}, Recomputed col: {test_col}\")\n",
    "\n",
    "            # Validate raster indexing\n",
    "            if 0 <= row < fuelmod.shape[0] and 0 <= col < fuelmod.shape[1]:\n",
    "                fuelmod_val = fuelmod[row, col]\n",
    "                print(f\"Fuelmod value at sampled point: {fuelmod_val}\")\n",
    "            else:\n",
    "                print(f\"Out of bounds for fuelmod raster. Row={row}, Col={col}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered for sample {idx + 1}/{num_samples}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02321e7-295c-405a-a47c-c31912ad5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sample_non_barren_points(\n",
    "        lon_array=lon_array,\n",
    "        lat_array=lat_array,\n",
    "        fuelmod=fuelmod,\n",
    "        num_samples=10,\n",
    "        elevation_transform=fuelmod_dataset.transform,\n",
    "        raster_crs=fuelmod_dataset.crs.to_string(),\n",
    "        raster_shape=elevation.shape\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f91af-d8d5-4358-a39e-ef987646c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the interpolator\n",
    "interp = Coord_to_index(degree = 2)\n",
    "interp.build(lon_grid, lat_grid) \n",
    "\n",
    "def calc_rhum(temp_K, mixing_ratio):\n",
    "    try:\n",
    "        # Constants\n",
    "        epsilon = 0.622\n",
    "        pressure_pa = 1000 * 100 # fixed until we can get the data\n",
    "        \n",
    "        # Saturation vapor pressure\n",
    "        es_hpa = 6.112 * np.exp((17.67 * (temp_K - 273.15)) / ((temp_K - 273.15) + 243.5))\n",
    "        es_pa = es_hpa * 100\n",
    "        \n",
    "        # Actual vapor pressure\n",
    "        e_pa = (mixing_ratio * pressure_pa) / (epsilon + mixing_ratio)\n",
    "        \n",
    "        # Relative humidity\n",
    "        rh = (e_pa / es_pa) * 100\n",
    "        return rh\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calc_rhum: temp_K={temp_K}, mixing_ratio={mixing_ratio}, error={e}\")\n",
    "        return np.nan\n",
    "\n",
    "def interpolate_data(lon_array, lat_array, date, interp, raster_crs, transform, raster_shape, elevation, slope, aspect, fuelmod, temp, rain, vapor, wind_u, wind_v, debug):\n",
    "    \"\"\"\n",
    "    Interpolate meteorological data and extract raster values for static features (batch processing).\n",
    "\n",
    "    Args:\n",
    "        lon_array (np.ndarray): Array of longitudes in WGS84.\n",
    "        lat_array (np.ndarray): Array of latitudes in WGS84.\n",
    "        date (datetime): Date associated with the interpolation.\n",
    "        interp: Interpolator object for meteorological data.\n",
    "        transform (Affine): Rasterio affine transform for static rasters.\n",
    "        raster_crs (str): CRS of the raster (e.g., \"EPSG:5070\").\n",
    "        elevation, slope, aspect, fuelmod: Static raster arrays.\n",
    "        temp, rain, vapor, wind_u, wind_v: Meteorological arrays.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries with interpolated meteorological data and static raster features.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Precompute rows and cols\n",
    "    rows, cols, lons, lats, mask = get_row_col(lon_array, lat_array, raster_crs, transform, raster_shape, debug)\n",
    "\n",
    "    for idx, (lon, lat, row, col) in enumerate(zip(lon_array, lat_array, rows, cols)):\n",
    "        try:\n",
    "            # Interpolate meteorological data\n",
    "            ia, ja = interp.evaluate(lon, lat)\n",
    "            i, j = np.round(ia).astype(int), np.round(ja).astype(int)\n",
    "\n",
    "            # Extract meteorological values\n",
    "            temp_val = temp[i, j]\n",
    "            rain_val = rain[i, j]\n",
    "            rhum_val = calc_rhum(temp[i, j], vapor[i, j])\n",
    "            wind_val = np.sqrt(wind_u[i, j]**2 + wind_v[i, j]**2)\n",
    "\n",
    "            # Check bounds and extract static raster values\n",
    "            if 0 <= row < elevation.shape[0] and 0 <= col < elevation.shape[1]:\n",
    "                elevation_val = elevation[row, col]\n",
    "                slope_val = slope[row, col]\n",
    "                aspect_val = aspect[row, col]\n",
    "                fuelmod_val = fuelmod[row, col]\n",
    "            else:\n",
    "                elevation_val, slope_val, aspect_val, fuelmod_val = np.nan, np.nan, np.nan, \"Out of bounds\"\n",
    "\n",
    "            # Build output dictionary\n",
    "            data_dict = {\n",
    "                'date': date,\n",
    "                'lon': lon,\n",
    "                'lat': lat,\n",
    "                'temp': temp_val,\n",
    "                'rain': rain_val,\n",
    "                'rhum': rhum_val,\n",
    "                'wind': wind_val,\n",
    "                'elevation': elevation_val,\n",
    "                'slope': slope_val,\n",
    "                'aspect': aspect_val,\n",
    "                'fuelmod': fuelmod_val,\n",
    "            }\n",
    "            results.append(data_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error interpolating and extracting data for lon={lon}, lat={lat}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example lon and lat arrays\n",
    "lon_array = np.array([-155.345, -155.678, -155.789])\n",
    "lat_array = np.array([19.123, 19.567, 19.890])\n",
    "date = \"2024-11-19\"  # Example date\n",
    "\n",
    "# Interpolate and extract data for all points\n",
    "results = interpolate_data(\n",
    "    lon_array=lon_array,\n",
    "    lat_array=lat_array,\n",
    "    date=date,\n",
    "    interp=interp,\n",
    "    raster_crs=elevation_dataset.crs.to_string(),\n",
    "    transform=elevation_transform,\n",
    "    raster_shape = elevation.shape,\n",
    "    elevation=elevation,\n",
    "    slope=slope,\n",
    "    aspect=aspect,\n",
    "    fuelmod=fuelmod,\n",
    "    temp=temp,\n",
    "    rain=rain,\n",
    "    vapor=vapor,\n",
    "    wind_u=wind_u,\n",
    "    wind_v=wind_v,\n",
    "    debug = False\n",
    ")\n",
    "\n",
    "# Print each result as a nicely formatted JSON\n",
    "for result in results:\n",
    "    converted_result = {key: (value.item() if isinstance(value, np.generic) else value) for key, value in result.items()}\n",
    "    print(converted_result)\n",
    "\n",
    "# Start timing and resource monitoring\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "start_cpu = process.cpu_percent(interval=None)\n",
    "start_mem = process.memory_info().rss  # in bytes\n",
    "\n",
    "# Run parallel interpolation\n",
    "data_interp = Parallel(n_jobs=-4)(\n",
    "    delayed(interpolate_data)(lon, lat) for lon, lat in zip(lon_array, lat_array)\n",
    ")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for easy handling\n",
    "df = pd.DataFrame(data_interp)\n",
    "\n",
    "# End timing and resource monitoring\n",
    "end_time = time.time()\n",
    "end_cpu = process.cpu_percent(interval=None)\n",
    "end_mem = process.memory_info().rss  # in bytes\n",
    "\n",
    "# Calculate the differences\n",
    "total_time = end_time - start_time\n",
    "cpu_usage = end_cpu - start_cpu\n",
    "memory_usage = end_mem - start_mem\n",
    "\n",
    "print(f\"Script runtime: {total_time:.2f} seconds\")\n",
    "print(f\"CPU usage change: {cpu_usage:.2f}%\")\n",
    "print(f\"Memory usage change: {memory_usage / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3cc2d-6db6-458c-965c-0e35ec662482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start timing and resource monitoring\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "start_cpu = process.cpu_percent(interval=None)\n",
    "start_mem = process.memory_info().rss  # in bytes\n",
    "\n",
    "# Run interpolation in a simple loop\n",
    "data_interp = []\n",
    "for lon, lat in zip(lon_array, lat_array):\n",
    "    data_interp.append(interpolate_data(lon, lat))\n",
    "    \n",
    "# Convert the list of dictionaries to a DataFrame for easy handling\n",
    "df = pd.DataFrame(data_interp)\n",
    "\n",
    "# End timing and resource monitoring\n",
    "end_time = time.time()\n",
    "end_cpu = process.cpu_percent(interval=None)\n",
    "end_mem = process.memory_info().rss  # in bytes\n",
    "\n",
    "# Calculate the differences\n",
    "total_time = end_time - start_time\n",
    "cpu_usage = end_cpu - start_cpu\n",
    "memory_usage = end_mem - start_mem\n",
    "\n",
    "print(f\"Script runtime: {total_time:.2f} seconds\")\n",
    "print(f\"CPU usage change: {cpu_usage:.2f}%\")\n",
    "print(f\"Memory usage change: {memory_usage / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648999ea-f249-4186-9a99-a50e48f0d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_data(lon_array[3], lat_array[5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafd7bf-db70-4c7e-a960-6d23aa52b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c597e8-ae93-42ca-8fcc-392042f1ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering of meteorology variables (i.e. averaging to determine effect on fuel moisture)\n",
    "print(dates_fire.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a0900-49c0-43f0-8b80-d34135fe6000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be8d2c-78da-49cd-8e1b-0b4ae696ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
